{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640f6f22",
   "metadata": {},
   "source": [
    "## 8.1 컨볼루션\n",
    "\n",
    "### 8.1.1 컨볼루션의 역할\n",
    "- 이산 컨볼루션(discrete convolution) : 2차원 이미지에 가중치 행렬을 스칼라곱을 수행하는 것으로 정의\n",
    "    * 가중치 행렬 = 커널\n",
    "  \n",
    "[특징]  \n",
    "- 지역성 : 이동된 커널 * 이미지 스칼라곱 \n",
    "- 평행이동 불변성 : 이미지 전 영역에 대해 동일한 커널 가중치\n",
    "- 적은 파라미터 : FC와는 달리 컨볼루션에서의 파라미터 수는 이미지 픽셀 수에 의존하지 않음  \n",
    "    -> 컨볼루션 커널 크기와 모델에서 얼마나 많은 컨볼루션 필터(출력 채널 수)를 쓰는지에 의존\n",
    "    \n",
    "## 8.2 컨볼루션 사용해보기\n",
    "- nn.Conv1d : 시계열용\n",
    "- nn.Conv2d : 이미지용\n",
    "- nn.Conv3d : 용적 데이터나 동영상용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c0bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5c55d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddc23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = r'C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47218818",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6e1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4398ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f015f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size = 3) # 입력 RGB 3, 출력 피처 16, kernel_size=(3, 3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec7cd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape # [출력, 입력, 커널, 커널]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a2f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape  # B X C X H X W, 출력의 픽셀 30으로 잘림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00811c43",
   "metadata": {},
   "source": [
    "### 8.2.1 경계 패딩하기\n",
    "- 패딩 여부 상관없이 weight, bias 크기는 변하지 않음\n",
    "- 컨볼루션과 이미지 크기 변경 문제를 별도로 분리해 기억해야 하는 것을 하나 줄이는데 도움\n",
    "- 컨볼루션 구조 자체에 더 신경쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7a84ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f84288e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 3, 3]), torch.Size([1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc9d31",
   "metadata": {},
   "source": [
    "### 8.2.2 컨볼루션으로 피처 찾아내기\n",
    "- CH07에서는 weight, bias 랜덤하게 초기화하고, 역전파를 통해 학습되는 파라미터\n",
    "- 컨볼루션은 직접 가중치 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a93c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias 0으로 제거해 교란 변수 배제\n",
    "with torch.no_grad() :\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "# 가중치에 상수값 넣어 출력에서의 각 픽셀이 자신의 이웃 픽셀에 대한 평균 가지게\n",
    "with torch.no_grad() :\n",
    "    conv.weight.fill_(1.0 / 9.0) # 3 X 3 이웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f812d9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]],\n",
       " \n",
       "          [[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]],\n",
       " \n",
       "          [[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]]]], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias, conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "288f916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "# 흐려진 이미지 : 각 출력의 픽셀은 자신의 주변 픽셀에 대한 평균이기에 출력 픽셀에서 이러한 상관관계 반영해 픽셀 간의 변화가 부드러워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdb9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "# 가로로 인접한 두 영역 사이의 수직 경계 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e313902",
   "metadata": {},
   "source": [
    "### 8.2.3 깊이와 풀링으로 한 단계 더 인식하기\n",
    "- 큰 이미지에서 작은 이미지로 다운샘플링  \n",
    " : 컨볼루션을 차례로 층층이 쌓으며 동시에 연속적인 컨볼루션 사이의 이미지 다운샘플링\n",
    "     * 네 개의 픽셀 평균하기 : 평균 풀링\n",
    "     * 네 개의 픽셀 중 최댓값 : 맥스 풀링 ; 데이터의 3/4 버림\n",
    "     * stride 하며 컨볼루션 수행하되, n번째 픽셀만 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89f2f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2) # 이미지 절반으로 줄임\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e66dd",
   "metadata": {},
   "source": [
    "### 8.2.4 우리의 신경망에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e992f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f748b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2), # 16 X 16 이미지로 다운샘플링\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2), # 8 X 8\n",
    "    \n",
    "        nn.Linear(8 * 8 * 8, 32),  # FC\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 2)) # 이진, 멀티 채널의 2차원 피처\n",
    "\n",
    "## nn.Sequential() : 각 모듈의 출력을 명시적으로 볼 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a889108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64cbcd3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))\n",
    "# 선형 계층의 크기는 8 * 8 * 8 = 512에 의존성을 가진다.\n",
    "# 모델의 용량을 높이려면 컨볼루션층의 출력 채널 수를 늘려 연결되는 선형 계층도 함께 키워줌\n",
    "# 런타임 오류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23bc9e",
   "metadata": {},
   "source": [
    "8 X 8 이미지를 512 요소를 가진 1차원 벡터로 차원 정보를 변경하면 오류 해결 !  \n",
    "마지막 nn.MaxPool2d(2)의 출력에 대해 **view**를 호출하면 해결할 수 있다.  \n",
    "하지만, nn.Sequential()은 각 모듈의 출력을 명시적으로 볼 수 없기에 불가능  => nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b88f6",
   "metadata": {},
   "source": [
    "## 8.3 nn.Module 서브클래싱하기\n",
    "* 계층 뒤에 다른 계층을 붙이는 것보다 더 복잡한 일을 수행하는 모델을 만들려면 유연성을 위해 nn.Sequential 대신 nn.Module 사용\n",
    "* 먼저 forward 함수 정의하여 모듈로 입력을 전달하고, 출력 반환하게 해야 한다.  \n",
    ": 모듈의 연산을 정의하는 영역\n",
    "* 표준 torch 연산을 사용하기만 한다면 자동미분 기능이 자동으로 역방향 경로를 만들어 주어 nn.Module에는 backward가 필요 없다.\n",
    "* super().__ init __()을 무조건 호출하기 !\n",
    "\n",
    "### 8.3.1 nn.Module로 정의된 우리의 신경망\n",
    "* 분류 신경망의 목적은 일반적으로 큰 수의 픽셀을 가진 이미지에서 출발해 정보를 압축해가면서 분류 클래스로 만들어 가는 것\n",
    "* 중간에 나타나는 값의 개수가 점점 줄어드는 모습  \n",
    "-> 컨볼루션의 채널 수가 점점 줄어들고, 풀링에서 픽셀 수가 줄어들며 선형 계층에서는 입력 차원보다 낮은 수의 차원을 출력한다. 빠르게 정보를 축소하는 패턴은 제한된 깊이의 신경망과 작은 이미지에 대해서는 잘 동작하지만, 신경망이 깊어질수록 감소는 천천히 일어난다.\n",
    "* 최초의 컨볼루션에는 입력 크기에 대해 출력 크기가 줄어들지 않는다.  \n",
    "만일 출력 픽셀 하나를 32개 채널을 가진 벡터로 본다면, 27개 채널(3채널 X 3 X 3 커널 크기를 가지는 컨볼루션)를 가지는 선형 변환에 해당하며 어느 정도 증가하는 셈이다. 따라서 첫 계층은 예외적으로 데이터의 전체 차원을 크게 증가시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc7200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8*8*8)  # B X N 벡터로\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b79c49",
   "metadata": {},
   "source": [
    "### 8.3.2 파이토치가 파라미터와 서브모듈을 유지하는 방법\n",
    "* 서브모듈은 list나 dict 인스턴스에 들어 있으면 안된다. 그렇지 않으면 옵티마이저가 서브모듈과 파라미터를 찾지 못한다. (nn.ModuleList, nn.ModuleDict는 있다.)\n",
    "* 훈련 과정이 실제 예측과 매우 다른 경우, 별도의 predict 함수를 만드는 게 낫다. 이런 메소드의 호출은 모듈 자체보다는 forward 메소드를 호출하는 것과 비슷해, hook 호출을 놓치게 되고 __call__을 호출하지 않기 때문에, JIT는 사용시 모듈 구조를 볼 수 없다.  \n",
    "- hook : 매 layer마다 print문을 통해 확인하지 않아도 각 층의 activation/gradient 값 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e9c4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090 [432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867915c",
   "metadata": {},
   "source": [
    "### 8.3.3 함수형 API\n",
    "* 파라미터가 없는 nn.Tanh, nn.MaxPool2d 같은 서브 모듈은 굳이 등록할 필요가 없다. 이들은 view 호출처럼 forward 함수에서 직접 호출하는 것이 더 쉽다.\n",
    "* '함수형'이란, 내부 상태가 없다는 의미, 즉 출력값이 전적으로 입력 인자에 의존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e24b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "811eb4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0759, 0.0127]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "model(img.unsqueeze(0)) # torch.Size([1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca7ca3",
   "metadata": {},
   "source": [
    "## 8.4 우리가 만든 컨볼루션 신경망 훈련시키기\n",
    "* 바깥 루프는 epoch 단위로 돌며, 안쪽 루프는 Dataset에서 배치를 만드는 DataLoader 단위로 돈다.\n",
    "\n",
    "1) 모델에 입력값을 넣고 (순방향 전달)\n",
    "2) 손실값을 계산하고 (순방향 전달)\n",
    "3) 이전 기울기 값을 0으로 리셋하고\n",
    "4) loss.backward()를 호출하여 모든 파라미터에 대한 손실값의 기울기를 계산한다.(역방향 전달)\n",
    "5) 이후 옵티마이저를 통해 손실값을 낮추도록 파라미터를 조정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "708828ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader) :\n",
    "    for epoch in range(1, n_epochs + 1) :\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader : # DataLoader가 만들어준 배치 안에서 데이터셋 순화\n",
    "            outputs = model(imgs) # 모델에 배치 넣어줌\n",
    "            \n",
    "            loss = loss_fn(outputs, labels) # 최소화하려는 손실값 계산\n",
    "            \n",
    "            optimizer.zero_grad() # 마지막에 이전 기울기 값 지움\n",
    "            loss.backward() # 역전파, 신경망이 학습할 모든 파라미터에 대한 기울기 계산\n",
    "            optimizer.step() # 모델 업데이트\n",
    "            \n",
    "            loss_train += loss.item() # epoch 동안 확인한 손실값 모두 더함. 기울기값 꺼내고자 .item() 사용해 손실값을 파이썬 수로 변환 !\n",
    "            \n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "            print('Epoch {}, Training loss {}'.format(\n",
    "                epoch, loss_train / len(train_loader)))  # 배치 단위의 평균 손실값 구하기 위해 훈련 데이터 로더의 길이로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48b7db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eb2d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.16623300351914327\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = 100, optimizer = optimizer, model = model,\n",
    "             loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a03978",
   "metadata": {},
   "source": [
    "### 8.4.1 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e778739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a4e4b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "def validate(model, train_loader, val_loader) :\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)] :\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "            for imgs, labels in loader :\n",
    "                outputs = model(imgs) # model에 배치 넣어줌\n",
    "                _, predicted = torch.max(outputs, dim=1) # 가장 높은 값을 가진 인덱스 출력\n",
    "                total += labels.shape[0] # 배치 크기만큼 증가\n",
    "                correct += int((predicted==labels).sum()) # 확률값이 가장 높았던 클래스와 레이블의 실측값 비교\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c18b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH08\\Data/'\n",
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c3955a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 모델 불러오기\n",
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34b54ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253707f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
