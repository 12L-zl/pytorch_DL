{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640f6f22",
   "metadata": {},
   "source": [
    "## 8.1 컨볼루션\n",
    "\n",
    "### 8.1.1 컨볼루션의 역할\n",
    "- 이산 컨볼루션(discrete convolution) : 2차원 이미지에 가중치 행렬을 스칼라곱을 수행하는 것으로 정의\n",
    "    * 가중치 행렬 = 커널\n",
    "  \n",
    "[특징]  \n",
    "- 지역성 : 이동된 커널 * 이미지 스칼라곱 \n",
    "- 평행이동 불변성 : 이미지 전 영역에 대해 동일한 커널 가중치\n",
    "- 적은 파라미터 : FC와는 달리 컨볼루션에서의 파라미터 수는 이미지 픽셀 수에 의존하지 않음  \n",
    "    -> 컨볼루션 커널 크기와 모델에서 얼마나 많은 컨볼루션 필터(출력 채널 수)를 쓰는지에 의존\n",
    "    \n",
    "## 8.2 컨볼루션 사용해보기\n",
    "- nn.Conv1d : 시계열용\n",
    "- nn.Conv2d : 이미지용\n",
    "- nn.Conv3d : 용적 데이터나 동영상용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c0bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5c55d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddc23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = r'C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47218818",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6e1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4398ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f015f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size = 3) # 입력 RGB 3, 출력 피처 16, kernel_size=(3, 3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec7cd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape # [출력, 입력, 커널, 커널]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a2f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape  # B X C X H X W, 출력의 픽셀 30으로 잘림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00811c43",
   "metadata": {},
   "source": [
    "### 8.2.1 경계 패딩하기\n",
    "- 패딩 여부 상관없이 weight, bias 크기는 변하지 않음\n",
    "- 컨볼루션과 이미지 크기 변경 문제를 별도로 분리해 기억해야 하는 것을 하나 줄이는데 도움\n",
    "- 컨볼루션 구조 자체에 더 신경쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7a84ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f84288e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 3, 3]), torch.Size([1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc9d31",
   "metadata": {},
   "source": [
    "### 8.2.2 컨볼루션으로 피처 찾아내기\n",
    "- CH07에서는 weight, bias 랜덤하게 초기화하고, 역전파를 통해 학습되는 파라미터\n",
    "- 컨볼루션은 직접 가중치 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a93c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias 0으로 제거해 교란 변수 배제\n",
    "with torch.no_grad() :\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "# 가중치에 상수값 넣어 출력에서의 각 픽셀이 자신의 이웃 픽셀에 대한 평균 가지게\n",
    "with torch.no_grad() :\n",
    "    conv.weight.fill_(1.0 / 9.0) # 3 X 3 이웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f812d9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]],\n",
       " \n",
       "          [[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]],\n",
       " \n",
       "          [[0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111],\n",
       "           [0.1111, 0.1111, 0.1111]]]], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias, conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "288f916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "# 흐려진 이미지 : 각 출력의 픽셀은 자신의 주변 픽셀에 대한 평균이기에 출력 픽셀에서 이러한 상관관계 반영해 픽셀 간의 변화가 부드러워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdb9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "# 가로로 인접한 두 영역 사이의 수직 경계 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e313902",
   "metadata": {},
   "source": [
    "### 8.2.3 깊이와 풀링으로 한 단계 더 인식하기\n",
    "- 큰 이미지에서 작은 이미지로 다운샘플링  \n",
    " : 컨볼루션을 차례로 층층이 쌓으며 동시에 연속적인 컨볼루션 사이의 이미지 다운샘플링\n",
    "     * 네 개의 픽셀 평균하기 : 평균 풀링\n",
    "     * 네 개의 픽셀 중 최댓값 : 맥스 풀링 ; 데이터의 3/4 버림\n",
    "     * stride 하며 컨볼루션 수행하되, n번째 픽셀만 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89f2f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2) # 이미지 절반으로 줄임\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e66dd",
   "metadata": {},
   "source": [
    "### 8.2.4 우리의 신경망에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b51a5288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f748b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2), # 16 X 16 이미지로 다운샘플링\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2), # 8 X 8\n",
    "    \n",
    "        nn.Linear(8 * 8 * 8, 32),  # FC\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 2)) # 이진, 멀티 채널의 2차원 피처\n",
    "\n",
    "## nn.Sequential() : 각 모듈의 출력을 명시적으로 볼 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a889108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64cbcd3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))\n",
    "# 선형 계층의 크기는 8 * 8 * 8 = 512에 의존성을 가진다.\n",
    "# 모델의 용량을 높이려면 컨볼루션층의 출력 채널 수를 늘려 연결되는 선형 계층도 함께 키워줌\n",
    "# 런타임 오류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f64e0",
   "metadata": {},
   "source": [
    "8 X 8 이미지를 512 요소를 가진 1차원 벡터로 차원 정보를 변경하면 오류 해결 !  \n",
    "마지막 nn.MaxPool2d(2)의 출력에 대해 **view**를 호출하면 해결할 수 있다.  \n",
    "하지만, nn.Sequential()은 각 모듈의 출력을 명시적으로 볼 수 없기에 불가능  => nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f38d0",
   "metadata": {},
   "source": [
    "## 8.3 nn.Module 서브클래싱하기\n",
    "* 계층 뒤에 다른 계층을 붙이는 것보다 더 복잡한 일을 수행하는 모델을 만들려면 유연성을 위해 nn.Sequential 대신 nn.Module 사용\n",
    "* 먼저 forward 함수 정의하여 모듈로 입력을 전달하고, 출력 반환하게 해야 한다.  \n",
    ": 모듈의 연산을 정의하는 영역\n",
    "* 표준 torch 연산을 사용하기만 한다면 자동미분 기능이 자동으로 역방향 경로를 만들어 주어 nn.Module에는 backward가 필요 없다.\n",
    "* super().__ init __()을 무조건 호출하기 !\n",
    "\n",
    "### 8.3.1 nn.Module로 정의된 우리의 신경망\n",
    "* 분류 신경망의 목적은 일반적으로 큰 수의 픽셀을 가진 이미지에서 출발해 정보를 압축해가면서 분류 클래스로 만들어 가는 것\n",
    "* 중간에 나타나는 값의 개수가 점점 줄어드는 모습  \n",
    "-> 컨볼루션의 채널 수가 점점 줄어들고, 풀링에서 픽셀 수가 줄어들며 선형 계층에서는 입력 차원보다 낮은 수의 차원을 출력한다. 빠르게 정보를 축소하는 패턴은 제한된 깊이의 신경망과 작은 이미지에 대해서는 잘 동작하지만, 신경망이 깊어질수록 감소는 천천히 일어난다.\n",
    "* 최초의 컨볼루션에는 입력 크기에 대해 출력 크기가 줄어들지 않는다.  \n",
    "만일 출력 픽셀 하나를 32개 채널을 가진 벡터로 본다면, 27개 채널(3채널 X 3 X 3 커널 크기를 가지는 컨볼루션)를 가지는 선형 변환에 해당하며 어느 정도 증가하는 셈이다. 따라서 첫 계층은 예외적으로 데이터의 전체 차원을 크게 증가시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc7200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8*8*8)  # B X N 벡터로\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06271cd8",
   "metadata": {},
   "source": [
    "### 8.3.2 파이토치가 파라미터와 서브모듈을 유지하는 방법\n",
    "* 서브모듈은 list나 dict 인스턴스에 들어 있으면 안된다. 그렇지 않으면 옵티마이저가 서브모듈과 파라미터를 찾지 못한다. (nn.ModuleList, nn.ModuleDict는 있다.)\n",
    "* 훈련 과정이 실제 예측과 매우 다른 경우, 별도의 predict 함수를 만드는 게 낫다. 이런 메소드의 호출은 모듈 자체보다는 forward 메소드를 호출하는 것과 비슷해, hook 호출을 놓치게 되고 __call__을 호출하지 않기 때문에, JIT는 사용시 모듈 구조를 볼 수 없다.  \n",
    "- hook : 매 layer마다 print문을 통해 확인하지 않아도 각 층의 activation/gradient 값 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e9c4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090 [432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501a0cf",
   "metadata": {},
   "source": [
    "### 8.3.3 함수형 API\n",
    "* 파라미터가 없는 nn.Tanh, nn.MaxPool2d 같은 서브 모듈은 굳이 등록할 필요가 없다. 이들은 view 호출처럼 forward 함수에서 직접 호출하는 것이 더 쉽다.\n",
    "* '함수형'이란, 내부 상태가 없다는 의미, 즉 출력값이 전적으로 입력 인자에 의존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e24b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "811eb4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0759, 0.0127]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "model(img.unsqueeze(0)) # torch.Size([1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a03cf4",
   "metadata": {},
   "source": [
    "## 8.4 우리가 만든 컨볼루션 신경망 훈련시키기\n",
    "* 바깥 루프는 epoch 단위로 돌며, 안쪽 루프는 Dataset에서 배치를 만드는 DataLoader 단위로 돈다.\n",
    "\n",
    "1) 모델에 입력값을 넣고 (순방향 전달)\n",
    "2) 손실값을 계산하고 (순방향 전달)\n",
    "3) 이전 기울기 값을 0으로 리셋하고\n",
    "4) loss.backward()를 호출하여 모든 파라미터에 대한 손실값의 기울기를 계산한다.(역방향 전달)\n",
    "5) 이후 옵티마이저를 통해 손실값을 낮추도록 파라미터를 조정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c60149d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader) :\n",
    "    for epoch in range(1, n_epochs + 1) :\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader : # DataLoader가 만들어준 배치 안에서 데이터셋 순화\n",
    "            outputs = model(imgs) # 모델에 배치 넣어줌\n",
    "            \n",
    "            loss = loss_fn(outputs, labels) # 최소화하려는 손실값 계산\n",
    "            \n",
    "            optimizer.zero_grad() # 마지막에 이전 기울기 값 지움\n",
    "            loss.backward() # 역전파, 신경망이 학습할 모든 파라미터에 대한 기울기 계산\n",
    "            optimizer.step() # 모델 업데이트\n",
    "            \n",
    "            loss_train += loss.item() # epoch 동안 확인한 손실값 모두 더함. 기울기값 꺼내고자 .item() 사용해 손실값을 파이썬 수로 변환 !\n",
    "            \n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "            print('Epoch {}, Training loss {}'.format(\n",
    "                epoch, loss_train / len(train_loader)))  # 배치 단위의 평균 손실값 구하기 위해 훈련 데이터 로더의 길이로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dab97148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a3f636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.16623300351914327\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = 100, optimizer = optimizer, model = model,\n",
    "             loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff936cd",
   "metadata": {},
   "source": [
    "### 8.4.1 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9616e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f2d7b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "def validate(model, train_loader, val_loader) :\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)] :\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad() : # 파라미터 업데이트 안해 기울기 필요 없음\n",
    "            for imgs, labels in loader :\n",
    "                outputs = model(imgs) # model에 배치 넣어줌\n",
    "                _, predicted = torch.max(outputs, dim=1) # 가장 높은 값을 가진 인덱스 출력\n",
    "                total += labels.shape[0] # 배치 크기만큼 증가\n",
    "                correct += int((predicted==labels).sum()) # 확률값이 가장 높았던 클래스와 레이블의 실측값 비교 (.item()과 같은 기능하는 int)\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9c5d3",
   "metadata": {},
   "source": [
    "### 8.4.2 모델 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c18b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH08\\Data/'\n",
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')\n",
    "# 모델의 모든 파라미터 저장, 모델 구조는 포함되어 있지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9277a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 모델 불러오기\n",
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7f48001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3372d8d",
   "metadata": {},
   "source": [
    "### 8.4.3 GPU에서 훈련시키기\n",
    "* .to 메소드 사용하여 데이터 로더에서 얻은 텐서와 파라미터를 GPU로 옮길 수 있다.\n",
    "* Module.to : 모듈 인스턴스 자체를 수정\n",
    "* Tensor.to : 새 텐서 반환\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader) :\n",
    "    for epoch in range(1, n_epochs + 1) :\n",
    "        loss_train = 0.0\n",
    "    \n",
    "    for imgs, labels in train_loader :\n",
    "        imgs = imgs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "        \n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        print('{} Epoch {}, Training loss {}'.format(\n",
    "            datetime.datetime.now(), epoch,\n",
    "            loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d979ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net().to(device=device) # 모델을 GPU로 옮긴다.\n",
    "# 모델이나 입력을 GPU로 옮기지 않으면, GPU와 CPU 입력을 섞어 실행하는 연산 지원하지 않는 파이토치라 '동일 디바이스에 존재하지 않음' 에러 발생\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba20335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict() # 차례대로 Dict에 append\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32241a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt', map_location=device))\n",
    "# GPU에 있던 가중치는 GPU로 복구\n",
    "# 나중에 어떤 디바이스에서 돌릴지 모르니, 신경망을 CPU로 옮긴 후 저장하든가, 파일에서 읽어들인 후 CPU로 옮기든가 해야함\n",
    "# 이를 간단하게 하려면, 가중치를 로딩할 때 파이토치가 기억하는 디바이스 정보를 덮어씀 -> torch.load 인자에 map_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a63be",
   "metadata": {},
   "source": [
    "## 8.5 모델 설계\n",
    "### 8.5.1 메모리 용량 늘리기 : 너비\n",
    "* 신경망의 너비 차원 : 신경망 계층 내의 뉴런 수 혹은 컨볼루션 채널 수에 해당하는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3c61e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 컨볼루션의 출력 채널 수를 더 크게 하고, 이어지는 계층도 맞춰 키워준다.\n",
    "# 더 길어진 벡터를 가지게 되니 이에 따라 완전 연결 계층으로 전환되는 forward 함수에도 반영됨\n",
    "class NetWidth(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "46d1c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3d48302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.083354063877824\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.9639, 'val': 0.8995}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NetWidth() #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(n_epochs=100, optimizer=optimizer, model=model, loss_fn=loss_fn, train_loader=train_loader)\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b61ce4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터를 init에 전달하고 너비를 파라미터화해서 forward함수에서 view를 호출할 때도 이를 고려한다.\n",
    "# 모델의 용량을 증가시킴\n",
    "class NetWidth(nn.Module) :\n",
    "    def __init__(self, n_chans1 = 32) :\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9afbb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.7035258112439684\n",
      "Accuracy train: 0.49\n",
      "Accuracy val: 0.50\n"
     ]
    }
   ],
   "source": [
    "model1 = NetWidth(n_chans1=32) # .to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "import collections\n",
    "all_acc_dict = collections.OrderedDict() # 차례대로 Dict에 append\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model1,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"width\"] = validate(model1, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "30b31ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38386\n",
      "38386\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(sum(p.numel() for p in model1.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf9184",
   "metadata": {},
   "source": [
    "용량이 클수록 모델이 다룰 수 있는 입력은 다양해진다. 하지만, 동시에 과적합할 가능성도 커진다. 파라미터가 많아져 입력에서 불필요한 부분까지 기억해버릴 수 있기 때문이다.\n",
    "\n",
    "샘플 수를 늘리거나 데이터가 충분하지 않다면 동일한 데이터를 인공적으로 수정해 증강시켜야 한다.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "  \n",
    "### 8.5.2 모델이 수렴하고 일반화하도록 돕는 방법 : 정규화\n",
    "* 모델 훈련은 중요한 두 단계를 거친다.\n",
    "    - 최적화 단계 : 훈련셋에 대해 손실값을 줄이는 경우\n",
    "    - 일반화 단계 : 모델이 훈련셋뿐 아니라 이전에 겪어보지 않은 검증셋 같은 데이터에 대해서도 동작하게 하는 것\n",
    "    \n",
    "      \n",
    "**1) 파라미터 제어하기 : 가중치 페널티**  \n",
    "일반화를 안정적으로 수행하기 위해 손실값에 정규화 항을 넣는다. 이 정규화 항을 조작하여 모델의 가중치가 상대적으로 작게 만든다. 즉, 훈련을 통해 증가할 수 있는 크기를 제한하는 것이다. 큰 가중치 값에 페널티를 부과하는 셈이다. 손실값은 다소 매끄러운 등고선 형태를 띠는데 개별 샘플에 맞춰 얻는 이득이 상대적으로 줄어들게 된다.\n",
    "- L1 정규화 : 모델의 모든 가중치 절댓값의 합 (라쏘)\n",
    "- L2 정규화 : 모델의 모든 가중치에 대한 제곱합 (릿지, 가중치 감쇠)  \n",
    "둘다 어떤 값으로 범위를 조정하는데, 이 값은 훈련 전 설정하는 하이퍼파라미터\n",
    "\n",
    "** 파이토치의 SGD 옵티마이저에는 이미 가중치 감쇠(L2) 파라미터(weight_decay)가 있고 2 * lambda 값을 가지며 업데이트 동작시 가중치 감쇠를 수행한다.   \n",
    "손실값에 가중치의 L2 정규화를 더한 것과 동일하므로, 자동미분과 손실값에서 항을 누적하는 작업을 할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2718b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader) :\n",
    "    for epoch in range(1, n_epochs + 1) :\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader :\n",
    "            #imgs = imgs.to(device=device)\n",
    "            #labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters()) # L1 정규화면 pow(2.0)을 abs()로 변환\n",
    "            loss = loss + l2_lambda + l2_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('Epoch {}, Training loss {}'.format(\n",
    "                epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f49e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 3.855360269926156\n",
      "Epoch 10, Training loss 0.6941726784797231\n",
      "Epoch 20, Training loss 0.6941726784797231\n",
      "Epoch 30, Training loss 0.6941726784797231\n",
      "Epoch 40, Training loss 0.6941726784797231\n",
      "Epoch 50, Training loss 0.6941726784797231\n",
      "Epoch 60, Training loss 0.6941726784797231\n",
      "Epoch 70, Training loss 0.6941726784797231\n",
      "Epoch 80, Training loss 0.6941726784797231\n",
      "Epoch 90, Training loss 0.6941726784797231\n",
      "Epoch 100, Training loss 0.6941726784797231\n",
      "Accuracy train: 0.50\n",
      "Accuracy val: 0.50\n"
     ]
    }
   ],
   "source": [
    "model = Net() #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f2ab8",
   "metadata": {},
   "source": [
    "**2) 입력 하나에 너무 의존하지 않기 : 드롭아웃**  \n",
    "* 훈련을 반복할 때마다 신경망의 뉴런 출력을 랜덤하게 0으로 만드는 작업\n",
    "* 매 훈련 때마다 조금씩 다른 뉴런의 토폴로지가 만들어지기에, 신경망이 각 입력 샘플을 암기하려는 기회를 줄이므로 과적합을 방지한다.\n",
    "* 모델이 피처를 만들려는 것을 교란해서 데이터 증강과 비슷한 효과를 내지만, 증강과 다르게 신경망 전체에 효과를 낸다.\n",
    "* 비선형 활성 함수와 선형 혹은 여러 계층의 컨볼루션 모듈 사이에 nn.Dropout 모듈 넣을 수 있다.\n",
    "* 어떤 입력이 0이 될지에 대한 확률을 지정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de891728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module) :\n",
    "    def __init__(self, n_chans1 = 32) :\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p = 0.4) # 어떤 입력이 0이 될지 확률 지정\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p = 0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5686b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.2364254861975172\n",
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout(n_chans1=32) #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd875c",
   "metadata": {},
   "source": [
    "훈련 중에 활성화되고 훈련이 끝난 모델을 제품으로 사용할 때는 그냥 통과하게 두거나 확률값에 0을 넣어준다. (추론시에는 드롭아웃 하면 안됨) Dropout 모듈의 train 속성을 통해 제어하면 된다. nn.Model 서브클래스에 대해 model.train() 혹은 model.eval() 호출로 전환 가능하다.\n",
    "  \n",
    "<br/>  \n",
    "  \n",
    "**3) 활성 함수 억제하기 : 배치 정규화**  \n",
    "* 입력 범위를 신경망의 활성 함수로 바꿔서 미니 배치가 원하는 분포를 가지게 하는 것\n",
    "* 미니 배치 샘플을 통해 중간 위치에서 얻은 평균과 표준편차를 사용하여 중간 입력값을 이동하고 범위를 바꾼다. 모델이 보는 개별 샘플이나 이로 인한 이후의 활성화 단계에서는 랜덤하게 뽑아 만들어진 미니 배치에서 통계에 의존한 값의 이동과 범위 조정이 반영되어 있는 상태다. 이 자체로 데이터 증강인 셈이다.\n",
    "* 배치 정규화가 드롭아웃을 할 필요를 없애거나 줄여준다.\n",
    "* 활성 함수의 입력 범위를 조정하는 것이므로, 선형 변환(컨볼루션) 뒤에 위치한다. (컨볼루션 -> 배치 정규화 -> 활성화(tanh) -> pooling)\n",
    "\n",
    "** 비선형 활성 함수를 사용하면, 함수의 임계 영역에서 입력이 활성 함수에 너무 많이 작용하여 기울기가 소실되고 훈련이 느려지는 상황을 피할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08ac855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module) :\n",
    "    def __init__(self, n_chans1 = 32) :\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features = n_chans1) # 입력 채널의 개수\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features = n_chans1//2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "868da9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.013112648032831064\n",
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32) #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e8af1",
   "metadata": {},
   "source": [
    "드롭아웃처럼 배치 정규화도 훈련 때와 추론 때는 각기 다르게 동작해야 한다. 추론 시 출력은 모델이 이미 봤던 다른 입력의 통계에 의존하는 특정 입력을 위한 것이 되어서는 안된다.\n",
    "\n",
    "미니 배치가 실행되면 현재의 미니 배치에 대한 평균과 표준편차를 구하는 것과 더불어 파이토치가 전체 데이터셋에 대한 평균과 표준편차도 대략적으로 업데이트한다. \n",
    "\n",
    "model.eval()을 명시하고, 모델이 배치 정규화 모듈을 가지는 경우 추정값을 고정하고 정규화에 사용하기만 한다.동작 중인 추정을 해제하고 다시 미니 배치 통계를 추정해가길 원한다면, 드롭아웃에서와 마찬가지로 model.train()을 호출한다.\n",
    "  \n",
    "  \n",
    "<br/>  \n",
    "\n",
    "### 8.5.3 더 복잡한 구조를 배우기 위해 깊이 파헤치기 : 깊이\n",
    "**스킵 커넥션**  \n",
    "* 모델의 깊이가 더해질수록 훈련은 수렴하기 어려워진다.\n",
    "* 곱셈이 체인이 길게 이어지는 경우 기울기값에 기여하는 파라미터가 사라져버려서 파라미터 같은 값들이 적절하게 업데이트되지 않기에 효과 없는 훈련 초래할 수 있다.(Vanishing)\n",
    "  \n",
    "=> 단순한 트릭으로 매우 깊은 신경망이어도 잘 훈련되는 **잔차 신경망**인 **레즈넷**을 만들었다\n",
    "* 스킵 커넥션 : 입력을 계층 블럭의 출력에 연결하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "48919d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계층 추가\n",
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32) # 풀링을 한 번 더해 4 X 4\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2) # ReLU  사용\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "401cacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.08072924349385842\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32) #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ef4caf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet처럼 스킵 커넥션 추가 : forward 함수 첫 번째 계층의 출력을 세 번째 계층의 입력에 추가\n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c5bce0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.06950008105130712\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32) #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd5c66",
   "metadata": {},
   "source": [
    "표준 Feed Forward 경로에 추가적으로 첫 번째 활성 함수의 출력을 마지막 부분의 입력으로 사용하는 것이다. : 아이덴티티 매핑  \n",
    "* 역전파에서, 스킵 커넥션 혹은 심층 신경망에서의 연속적인 스킵 커넥션은 깊은 쪽에 있는 파라미터를 손실값에 연결하는 역할을 한다. 이로 인해 체인으로 길게 연결된 여러 다른 연산들로 곱해질 기회가 줄어들고, 파라미터에 대한 손실값의 편미분으로 손실값에 대한 기울기에 더욱 직접적으로 관여한다.\n",
    "* 스킵 커넥션은 특히 훈련 초반 단계에서 수렴에 도움이 된다.\n",
    "* 스킵 커넥션은 잔차 신경망의 손실값 분포는 동일한 깊이나 너비를 가진 Feed Forward NN보다 훨씬 부드럽다.\n",
    "  \n",
    "<br/>\n",
    "\n",
    "**파이토치로 매우 깊은 모델 만들기**\n",
    "* 100개 이상의 계층을 담는다면,   \n",
    "(Conv2d + ReLU + Conv2d) + skip connection 같은 빌딩 블럭을 정의한 후,  \n",
    "for 루프를 사용하여 신경망을 동적으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bade8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # BatchNorm 계층은 편향값의 효과를 상쇄\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # 커스텀 초기화 : 표준편차를 가지는 표준 랜덤 요소로 초기화\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5) # 배치 정규화는 기본값으로 평균 0과 분산 0.5를 가지는 분포 출력으로 초기화\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias) # bias는 0으로 텐서 채움\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf20085",
   "metadata": {},
   "source": [
    "깊은 모델을 만들기로 했으므로 블럭에 배치 정규화를 넣어 훈련 도중에 기울기 값이 없어지는 것을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30e60cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)])) # ResBlock()을 n_blocks번 반복하여 list로 만든 후, 이를 nn.Sequential에 전달\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59a00e",
   "metadata": {},
   "source": [
    "init에서 ResBlock 인스턴스 리스트를 포함한 nn.Sequential(한 블럭의 출력을 다음 블럭의 입력으로 사용 가능)을 만든다. 또한 블럭 내의 모든 파라미터를 Net이 볼 수 있게 해준다. 이렇게 만든 sequential을 forward에서 호출해 100개의 블럭을 거쳐 출력을 만든다.\n",
    "\n",
    "** resblocks에서 맨 앞의 * 는 iterable의 각 항목을 함수의 인자로 전달하는 unpacking 연산자로, 리스트의 각 요소가 개벽적인 인자로 전달된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f3696295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.024132309595692406\n",
      "Accuracy train: 0.98\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100) #.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96b53d",
   "metadata": {},
   "source": [
    "### 8.5.4 모델의 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d886ba79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('width', {'train': 0.4888, 'val': 0.5005}),\n",
       "             ('l2 reg', {'train': 0.5, 'val': 0.5}),\n",
       "             ('dropout', {'train': 0.8975, 'val': 0.886}),\n",
       "             ('batch_norm', {'train': 0.9861, 'val': 0.8705}),\n",
       "             ('depth', {'train': 0.965, 'val': 0.9025}),\n",
       "             ('res', {'train': 0.9685, 'val': 0.9035}),\n",
       "             ('res deep', {'train': 0.9842, 'val': 0.8765})])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f50ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width =0.3\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()),\n",
    "           rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1d312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
