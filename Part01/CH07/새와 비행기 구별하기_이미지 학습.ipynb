{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07169444",
   "metadata": {},
   "source": [
    "## 7.1 작은 이미지 모아 놓은 데이터셋\n",
    "\n",
    "### 7.1.1 CIFAR-10 다운로드\n",
    "- 32 X 32 크기의 컬러(RGB) 이미지 6만개\n",
    "- 1에서 10까지의 정수로 레이블\n",
    "    * 0 : 비행기 / 1 : 자동차 / 2 : 새 / 3 : 고양이 / 4 : 사슴 / 5 : 강아지 / 6 : 개구리 / 7 : 말 / 8 : 배 / 9 : 트럭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f294280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0f23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 170498071/170498071 [00:14<00:00, 11704315.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/cifar-10-python.tar.gz to C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = r'C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치 딥러닝 마스터\\Minji\\Part01\\CH07\\Data/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True) # 훈련데이터용 데이터 객체 만듦\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6f4398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " typing.Generic,\n",
       " object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e61be",
   "metadata": {},
   "source": [
    "### 7.1.2 데이터셋 클래스\n",
    "- torch.utils.data.Dataset의 서브클래스\n",
    "    * Dataset은 데이터를 직접 들고 있지는 않지만, __ len __, __getitem__을 통해 접근할 수 있다.\n",
    "        - __ len__ : 데이터셋의 아이템 수 반환\n",
    "        - __ getitem__ : 샘플과 레이블(정수 인덱스)로 이루어진 아이템 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02bc82ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f754ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32>, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset에 __getitem__메소드가 구현되어 있으므로 개별 아이템에 접근할 때 표준 서브스크립트에 해당하는 색인용 튜플과 리스트 사용 가능\n",
    "# 자동차에 해당하는 정수값이 1인 PIL 형식의 이미지\n",
    "img, label = cifar10[99]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfa1a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "class_names[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f65d9f",
   "metadata": {},
   "source": [
    "### 7.1.3 데이터 변환\n",
    "- matplotlib으로 출력하기 위헤 permute() 사용하여 C X H X W를 H X W X C 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9f07be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AugMix',\n",
       " 'AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'ElasticTransform',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional_pil',\n",
       " '_functional_tensor',\n",
       " '_presets',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIL 이미지 -> 텐서로 변환\n",
    "from torchvision import transforms\n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b3e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape # C X H X W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2bbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07bf6aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t) # PIL 타입이 아닌 tensor로 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce557fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f0b9f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.min(), img_t.max() # PIL이미지는 0 ~ 255(채널당 8비트) 범위, ToTensor 변환으로 데이터가 채널당 32비트 부동소수점 형태가 되면서 0.0에서 1.0 사이로 범위 줄어듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d50a60",
   "metadata": {},
   "source": [
    "### 7.1.4 데이터 정규화\n",
    "- -1 ~ +1 혹은 -2 ~ +2 사이에서 선형인 활성 함수 택하고 데이터를 같은 범위에서 평균을 가지게 한다면 뉴런은 0이 아닌 기울기를 가지게 되므로 빨리 학습할 수 있다.\n",
    "- 각 채널을 정규화해 동일한 분산을 가지게 된다면, 채널 정보가 동일한 학습률로 경사 하강을 통해 섞인다.\n",
    "- transforms.Normalize ; mean, stdev 계산하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af049115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d99022a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4914, 0.4822, 0.4465]), (0.4914, 0.4822, 0.4465))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "mean_t = tuple(np.round(mean.tolist(),4))\n",
    "mean, mean_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d278c644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51200000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b6520d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2470, 0.2435, 0.2616]), (0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = imgs.view(3, -1).std(dim=1)\n",
    "std_t = tuple(np.round(std.tolist(),4))\n",
    "std, std_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66912761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((mean_t), (std_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43410ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean_t),(std_t))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2424f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean_t),(std_t))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a480cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t, _ = transformed_cifar10[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556917d",
   "metadata": {},
   "source": [
    "## 7.2 새와 비행기 구별하기\n",
    "\n",
    "### 7.2.1 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e063256",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1} # 0 : 비행기, 2 : 새\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a8dd18b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar2), len(cifar2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b44b2",
   "metadata": {},
   "source": [
    "### 7.2.2 완전 연결 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0310b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e5fedb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2 # 비행기, 새"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb29ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 512,), # 32 * 32 * 3 = 3072\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c744f7",
   "metadata": {},
   "source": [
    "### 7.2.3 분류기의 출력\n",
    "- 소프트맥스\n",
    "    * 출력값의 요소가 가질 수 있는 값은 [0.0, 1.0] 범위로 제한된다   \n",
    "    -> 확률은 0보다 작거나 1보다 클 수 없으므로\n",
    "    * 모든 출력 요소의 값의 합은 1.0이다.  \n",
    "    -> 결과는 항상 새 아니면 비행기라고 가정\n",
    "### 7.2.4 출력을 확률로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e5e13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x) :\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3354a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d04170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 2),\n",
    "        nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7b0c37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "img.shape, _ # label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ef50edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력이 3072개의 피처를 가지고 있으며 0번 차원을 따라 배치로 이뤄지는 데이터\n",
    "# 3 X 32 X 32 이미지를 1차원 텐서로 만들고 추가 차원을 0번 포지션에 넣는다.\n",
    "img_batch = img.view(-1).unsqueeze(0)\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dee617a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5953, 0.4047]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out # 선형계층의 가중치, 편향값 훈련되지 않고, 랜덤 값으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c4a5e083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argmax : 제일 높은 확률에 대한 인덱스 -> 레이블 구함\n",
    "_, index = torch.max(out, dim=1)\n",
    "index # 틀림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863dda2",
   "metadata": {},
   "source": [
    "### 7.2.5 분류를 위한 손실값\n",
    "- 정답 클래스 out[class_index]와 관련된 확률 극대화  \n",
    "-> 가능도 :정답 클래스에 대한 확률 수치  \n",
    "    * 가능도 낮을 때, 즉 다른 클래스의 확률이 매우 높을 때 값이 커지는 손실 함수 필요\n",
    "    * 가능도가 다른 클래스보다 높으면 손실값은 낮아야 한다.  \n",
    "=> NLL(Negative Log Likelihood) = -sum(log(out_i[c_i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d33f9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 확률의 텐서를 받는 nn.NLLLoss -> 입력을 확률의 로그값으로 받으면 확률이 0에 가까울 경우 문제\n",
    "# => nn.Softmax() 대신 nn.LogSoftmax() 사용\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 2),\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "809a2fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9674, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dfedb",
   "metadata": {},
   "source": [
    "### 7.2.6 분류기 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "29be1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 6.434373\n",
      "Epoch: 2, Loss: 17.315269\n",
      "Epoch: 3, Loss: 6.000750\n",
      "Epoch: 4, Loss: 6.839866\n",
      "Epoch: 5, Loss: 4.823521\n",
      "Epoch: 6, Loss: 9.278174\n",
      "Epoch: 7, Loss: 16.225620\n",
      "Epoch: 8, Loss: 2.514620\n",
      "Epoch: 9, Loss: 0.002799\n",
      "Epoch: 10, Loss: 7.355705\n",
      "Epoch: 11, Loss: 4.979154\n",
      "Epoch: 12, Loss: 15.045765\n",
      "Epoch: 13, Loss: 7.951532\n",
      "Epoch: 14, Loss: 7.577113\n",
      "Epoch: 15, Loss: 15.258724\n",
      "Epoch: 16, Loss: 12.777469\n",
      "Epoch: 17, Loss: 10.226240\n",
      "Epoch: 18, Loss: 8.091814\n",
      "Epoch: 19, Loss: 12.951021\n",
      "Epoch: 20, Loss: 4.535486\n",
      "Epoch: 21, Loss: 13.321553\n",
      "Epoch: 22, Loss: 4.383187\n",
      "Epoch: 23, Loss: 7.040248\n",
      "Epoch: 24, Loss: 1.412455\n",
      "Epoch: 25, Loss: 4.507382\n",
      "Epoch: 26, Loss: 7.903306\n",
      "Epoch: 27, Loss: 11.454595\n",
      "Epoch: 28, Loss: 0.782271\n",
      "Epoch: 29, Loss: 0.091054\n",
      "Epoch: 30, Loss: 2.631381\n",
      "Epoch: 31, Loss: 4.315165\n",
      "Epoch: 32, Loss: 0.000543\n",
      "Epoch: 33, Loss: 3.160376\n",
      "Epoch: 34, Loss: 10.463407\n",
      "Epoch: 35, Loss: 4.504496\n",
      "Epoch: 36, Loss: 2.021827\n",
      "Epoch: 37, Loss: 5.648680\n",
      "Epoch: 38, Loss: 9.216259\n",
      "Epoch: 39, Loss: 3.901215\n",
      "Epoch: 40, Loss: 1.522445\n",
      "Epoch: 41, Loss: 8.079819\n",
      "Epoch: 42, Loss: 3.954383\n",
      "Epoch: 43, Loss: 2.737644\n",
      "Epoch: 44, Loss: 3.291663\n",
      "Epoch: 45, Loss: 12.868998\n",
      "Epoch: 46, Loss: 12.301942\n",
      "Epoch: 47, Loss: 4.674218\n",
      "Epoch: 48, Loss: 5.543658\n",
      "Epoch: 49, Loss: 10.045831\n",
      "Epoch: 50, Loss: 1.670454\n",
      "Epoch: 51, Loss: 10.283358\n",
      "Epoch: 52, Loss: 5.471765\n",
      "Epoch: 53, Loss: 10.731573\n",
      "Epoch: 54, Loss: 11.989758\n",
      "Epoch: 55, Loss: 11.091637\n",
      "Epoch: 56, Loss: 6.923275\n",
      "Epoch: 57, Loss: 14.592337\n",
      "Epoch: 58, Loss: 12.319016\n",
      "Epoch: 59, Loss: 6.479873\n",
      "Epoch: 60, Loss: 4.353077\n",
      "Epoch: 61, Loss: 5.772213\n",
      "Epoch: 62, Loss: 11.461337\n",
      "Epoch: 63, Loss: 11.218088\n",
      "Epoch: 64, Loss: 6.837375\n",
      "Epoch: 65, Loss: 3.800491\n",
      "Epoch: 66, Loss: 0.001615\n",
      "Epoch: 67, Loss: 6.068801\n",
      "Epoch: 68, Loss: 6.992395\n",
      "Epoch: 69, Loss: 4.434329\n",
      "Epoch: 70, Loss: 11.221608\n",
      "Epoch: 71, Loss: 10.552855\n",
      "Epoch: 72, Loss: 11.382199\n",
      "Epoch: 73, Loss: 6.258723\n",
      "Epoch: 74, Loss: 4.228119\n",
      "Epoch: 75, Loss: 2.128421\n",
      "Epoch: 76, Loss: 0.086648\n",
      "Epoch: 77, Loss: 11.056211\n",
      "Epoch: 78, Loss: 0.226866\n",
      "Epoch: 79, Loss: 5.435720\n",
      "Epoch: 80, Loss: 14.650733\n",
      "Epoch: 81, Loss: 4.306685\n",
      "Epoch: 82, Loss: 0.193987\n",
      "Epoch: 83, Loss: 8.647223\n",
      "Epoch: 84, Loss: 7.436965\n",
      "Epoch: 85, Loss: 12.838311\n",
      "Epoch: 86, Loss: 5.677036\n",
      "Epoch: 87, Loss: 1.857408\n",
      "Epoch: 88, Loss: 5.923993\n",
      "Epoch: 89, Loss: 0.082561\n",
      "Epoch: 90, Loss: 0.748036\n",
      "Epoch: 91, Loss: 13.460581\n",
      "Epoch: 92, Loss: 6.867589\n",
      "Epoch: 93, Loss: 0.912602\n",
      "Epoch: 94, Loss: 3.591004\n",
      "Epoch: 95, Loss: 0.064557\n",
      "Epoch: 96, Loss: 6.281099\n",
      "Epoch: 97, Loss: 0.303185\n",
      "Epoch: 98, Loss: 19.737333\n",
      "Epoch: 99, Loss: 7.215333\n",
      "Epoch: 100, Loss: 0.002078\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs=100\n",
    "\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "    for img, label in cifar2 :  # 한번에 하나의 샘플 평가\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "# 들쭉날쭉한 loss . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e898ec",
   "metadata": {},
   "source": [
    "각 에포크마다 샘플을 섞은 후 한 번에 혹은 여러 개의 샘플에 대해 기울기 평가하면 경사 하강에 랜덤한 효과 줄 수 있다.  \n",
    "\n",
    "  \n",
    "torch.utils.data 모듈에는 미니 배치의 데이터를 섞거나 구조화하는 작업을 돕는 DataLoader 클래스 있다.   \n",
    "-> 미니 배치에 포함될 샘플 가져올 때, 각 에포크마다 데이터 섞은 후 고르게 샘플링함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d7babe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3259f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.373018\n",
      "Epoch: 2, Loss: 0.408726\n",
      "Epoch: 3, Loss: 0.357243\n",
      "Epoch: 4, Loss: 0.305634\n",
      "Epoch: 5, Loss: 0.187234\n",
      "Epoch: 6, Loss: 0.785617\n",
      "Epoch: 7, Loss: 0.814715\n",
      "Epoch: 8, Loss: 0.425080\n",
      "Epoch: 9, Loss: 0.289232\n",
      "Epoch: 10, Loss: 0.322380\n",
      "Epoch: 11, Loss: 0.491398\n",
      "Epoch: 12, Loss: 0.453093\n",
      "Epoch: 13, Loss: 0.412017\n",
      "Epoch: 14, Loss: 0.247110\n",
      "Epoch: 15, Loss: 0.497091\n",
      "Epoch: 16, Loss: 0.724647\n",
      "Epoch: 17, Loss: 0.315986\n",
      "Epoch: 18, Loss: 0.470300\n",
      "Epoch: 19, Loss: 0.172258\n",
      "Epoch: 20, Loss: 0.177786\n",
      "Epoch: 21, Loss: 0.230062\n",
      "Epoch: 22, Loss: 0.306673\n",
      "Epoch: 23, Loss: 0.302494\n",
      "Epoch: 24, Loss: 0.422032\n",
      "Epoch: 25, Loss: 0.134306\n",
      "Epoch: 26, Loss: 0.266599\n",
      "Epoch: 27, Loss: 0.165318\n",
      "Epoch: 28, Loss: 0.381498\n",
      "Epoch: 29, Loss: 0.336131\n",
      "Epoch: 30, Loss: 0.112090\n",
      "Epoch: 31, Loss: 0.109310\n",
      "Epoch: 32, Loss: 0.297195\n",
      "Epoch: 33, Loss: 0.266919\n",
      "Epoch: 34, Loss: 0.160415\n",
      "Epoch: 35, Loss: 0.152047\n",
      "Epoch: 36, Loss: 0.080195\n",
      "Epoch: 37, Loss: 0.067546\n",
      "Epoch: 38, Loss: 0.079039\n",
      "Epoch: 39, Loss: 0.095422\n",
      "Epoch: 40, Loss: 0.253430\n",
      "Epoch: 41, Loss: 0.133393\n",
      "Epoch: 42, Loss: 0.183212\n",
      "Epoch: 43, Loss: 0.286666\n",
      "Epoch: 44, Loss: 0.046894\n",
      "Epoch: 45, Loss: 0.189310\n",
      "Epoch: 46, Loss: 0.143637\n",
      "Epoch: 47, Loss: 0.912227\n",
      "Epoch: 48, Loss: 0.034363\n",
      "Epoch: 49, Loss: 0.093417\n",
      "Epoch: 50, Loss: 0.023215\n",
      "Epoch: 51, Loss: 0.065671\n",
      "Epoch: 52, Loss: 0.061411\n",
      "Epoch: 53, Loss: 0.054304\n",
      "Epoch: 54, Loss: 0.182842\n",
      "Epoch: 55, Loss: 0.297646\n",
      "Epoch: 56, Loss: 0.056206\n",
      "Epoch: 57, Loss: 0.041924\n",
      "Epoch: 58, Loss: 0.065923\n",
      "Epoch: 59, Loss: 0.104005\n",
      "Epoch: 60, Loss: 0.100479\n",
      "Epoch: 61, Loss: 0.100568\n",
      "Epoch: 62, Loss: 0.031243\n",
      "Epoch: 63, Loss: 0.056038\n",
      "Epoch: 64, Loss: 0.057708\n",
      "Epoch: 65, Loss: 0.037087\n",
      "Epoch: 66, Loss: 0.057206\n",
      "Epoch: 67, Loss: 0.018380\n",
      "Epoch: 68, Loss: 0.043491\n",
      "Epoch: 69, Loss: 0.033692\n",
      "Epoch: 70, Loss: 0.086978\n",
      "Epoch: 71, Loss: 0.018623\n",
      "Epoch: 72, Loss: 0.053233\n",
      "Epoch: 73, Loss: 0.027008\n",
      "Epoch: 74, Loss: 0.033473\n",
      "Epoch: 75, Loss: 0.059615\n",
      "Epoch: 76, Loss: 0.015944\n",
      "Epoch: 77, Loss: 0.019933\n",
      "Epoch: 78, Loss: 0.051077\n",
      "Epoch: 79, Loss: 0.045063\n",
      "Epoch: 80, Loss: 0.018689\n",
      "Epoch: 81, Loss: 0.011834\n",
      "Epoch: 82, Loss: 0.023774\n",
      "Epoch: 83, Loss: 0.019511\n",
      "Epoch: 84, Loss: 0.022944\n",
      "Epoch: 85, Loss: 0.018920\n",
      "Epoch: 86, Loss: 0.017115\n",
      "Epoch: 87, Loss: 0.006129\n",
      "Epoch: 88, Loss: 0.001216\n",
      "Epoch: 89, Loss: 0.012387\n",
      "Epoch: 90, Loss: 0.027698\n",
      "Epoch: 91, Loss: 0.027944\n",
      "Epoch: 92, Loss: 0.009314\n",
      "Epoch: 93, Loss: 0.021731\n",
      "Epoch: 94, Loss: 0.012759\n",
      "Epoch: 95, Loss: 0.007771\n",
      "Epoch: 96, Loss: 0.014304\n",
      "Epoch: 97, Loss: 0.020357\n",
      "Epoch: 98, Loss: 0.011997\n",
      "Epoch: 99, Loss: 0.017669\n",
      "Epoch: 100, Loss: 0.003420\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 2),\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs + 1) :\n",
    "    for imgs, labels in train_loader : # imgs.shape : 64 X 3 X 32 X 32\n",
    "        batch_size = imgs.shape[0] # 64\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "    \n",
    "# 점점 떨어지는 loss . . . !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c059bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5c348e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad() :\n",
    "    for imgs, labels in val_loader :\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0] # 64 * 31 + 16 = 2000\n",
    "        correct += int((predicted == labels).sum()) # 맞춘 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "faca1202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total # 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18960f89",
   "metadata": {},
   "source": [
    "nn.LogSoftmax() + nn.NLLLoss() = nn.CrossEntropyLoss()  \n",
    "    * nn.NLLLoss() : 입력으로 로그 확률 예측 받음  \n",
    "    * nn.CrossEntropyLoss() : 입력으로 점수 받음(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6675fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 1024),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 128),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0f1377b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter 개수 -> requires_grad=True 여야함\n",
    "numel_list = [p.numel() for p in model.parameters() if p.requires_grad==True]\n",
    "sum(numel_list), numel_list  # (가중치, 편향) * 4개의 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "095773e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3146752, [3145728, 1024])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list1 = [p.numel() for p in nn.Linear(3072, 1024).parameters() if p.requires_grad==True]\n",
    "sum(numel_list1), numel_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7296fb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1573376, [1572864, 512])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list2 = [p.numel() for p in nn.Linear(3072, 512).parameters() if p.requires_grad==True]\n",
    "sum(numel_list2), numel_list2 # 파라미터 수 적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0a81896a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b92564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
